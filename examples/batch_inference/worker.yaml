envs:
  DATA_BUCKET: r2://skypilot-lmsys-chat-1m
  # The name of the model to be used for inference.
  MODEL_NAME: meta-llama/Meta-Llama-3.1-8B-Instruct
  HF_TOKEN:
  # Output bucket to use for storing the results
  OUTPUT_BUCKET:
  # The file path that contains the paths to data chunks to be processed.
  DATA_CHUNK_FILE:


file_mounts:
  /data:
    source: $DATA_BUCKET
    mode: MOUNT

  /output:
    source: $OUTPUT_BUCKET
    mode: MOUNT

  /chunks.txt: $DATA_CHUNK_FILE

workdir: .

resources:
  accelerators: L4:1

setup: |
  pip install vllm


run: |
  python worker.py \
    --data-chunk-file /chunks.txt \
    --model-name $MODEL_NAME \
    --num-gpus $SKYPILOT_NUM_GPUS_PER_NODE
